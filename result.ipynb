{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, BatchNormalization, LeakyReLU, Multiply\n",
    "from keras.applications import DenseNet201\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание путей до датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_dcm(root_dir):\n",
    "    \"\"\"\n",
    "    Функция для сбора информации о файлах .dcm в указанном каталоге и его подкаталогах.\n",
    "\n",
    "    Аргументы:\n",
    "    root_dir (str): Путь к корневому каталогу для поиска файлов.\n",
    "\n",
    "    Возвращает:\n",
    "    pd.DataFrame: DataFrame с информацией о пути к файлам и именами родительских директорий.\n",
    "    \"\"\"\n",
    "    # Список для хранения данных о файлах .dcm\n",
    "    data = []\n",
    "\n",
    "    # Проход по всем подкаталогам и файлам\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.dcm'):\n",
    "                # Полный путь к файлу\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                # Извлечение списка директорий в пути\n",
    "                dirs = file_path.split(os.sep)\n",
    "\n",
    "                # Извлечение нужного уровня родительской папки\n",
    "                if len(dirs) >= 3:\n",
    "                    # Проверка, что структура разрешает нам взять третий элемент\n",
    "                    parent_folder = dirs[-3]\n",
    "                else:\n",
    "                    parent_folder = None\n",
    "\n",
    "                # Добавление информации в список\n",
    "                data.append({'path_name': parent_folder, 'file_path': file_path})\n",
    "\n",
    "    # Создание DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Добавление столбца index\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Использование функции для медиц-й базы\n",
    "medimp_directory = '/Users/vadimkirsanov/Desktop/MIPT_DS/Python_coding_data/chest_xray_hac/ds_medimp_train_good/block_0000_anon'\n",
    "df_medimp = dataframe_dcm(medimp_directory)\n",
    "df_medimp = df_medimp.sort_values(by='path_name')\n",
    "\n",
    "# Использование функции для базы ключицы\n",
    "clav_fracture_directory = '/Users/vadimkirsanov/Desktop/MIPT_DS/Python_coding_data/chest_xray_hac/ds_clav_fracture_train_good/block_0000_anon'\n",
    "df_clav_fracture = dataframe_dcm(clav_fracture_directory)\n",
    "df_clav_fracture = df_clav_fracture.sort_values(by='path_name')\n",
    "\n",
    "# Печать результатов\n",
    "print(df_medimp)\n",
    "print(df_clav_fracture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение путей к файлам Excel\n",
    "medimp_excel_path = '/Users/vadimkirsanov/Desktop/MIPT_DS/Python_coding_data/chest_xray_hac/ds_medimp_train_good/block_0000_anon/block_0000_anon.xlsx'\n",
    "clav_fracture_excel_path = '/Users/vadimkirsanov/Desktop/MIPT_DS/Python_coding_data/chest_xray_hac/ds_clav_fracture_train_good/block_0000_anon/block_0000_anon.xlsx'\n",
    "\n",
    "# Чтение данных из файлов Excel в DataFrame\n",
    "medimp_data = pd.read_excel(medimp_excel_path)\n",
    "clav_fracture_data = pd.read_excel(clav_fracture_excel_path)\n",
    "\n",
    "# Сортировка данных по столбцу `study_instance_anon`\n",
    "medimp_data_sorted = medimp_data.sort_values(by='study_instance_anon')\n",
    "clav_fracture_data_sorted = clav_fracture_data.sort_values(by='study_instance_anon')\n",
    "\n",
    "# Вывод первых нескольких строк каждого отсортированного DataFrame\n",
    "print(\"Medimp Data (Sorted):\")\n",
    "print(medimp_data_sorted.head())\n",
    "\n",
    "print(\"\\nClavicle Fracture Data (Sorted):\")\n",
    "print(clav_fracture_data_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем данные 'medimp_data_sorted' с 'df_medimp' на основе столбцов 'path_name' и 'study_instance_anon'\n",
    "df_medimp = df_medimp.merge(\n",
    "    medimp_data_sorted[['study_instance_anon', 'pathology']],\n",
    "    how='left',\n",
    "    left_on='path_name',\n",
    "    right_on='study_instance_anon'\n",
    ")\n",
    "\n",
    "# Удаляем временный столбец 'study_instance_anon' после слияния\n",
    "df_medimp.drop(columns=['study_instance_anon'], inplace=True)\n",
    "\n",
    "# Удаляем временный столбец 'index', если он существует\n",
    "df_medimp.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Объединяем данные 'clav_fracture_data_sorted' с 'df_clav_fracture' на основе столбцов 'path_name' и 'study_instance_anon'\n",
    "df_clav_fracture = df_clav_fracture.merge(\n",
    "    clav_fracture_data_sorted[['study_instance_anon', 'pathology']],\n",
    "    how='left',\n",
    "    left_on='path_name',\n",
    "    right_on='study_instance_anon'\n",
    ")\n",
    "\n",
    "# Удаляем временный столбец 'study_instance_anon' после слияния\n",
    "df_clav_fracture.drop(columns=['study_instance_anon'], inplace=True)\n",
    "\n",
    "# Удаляем временный столбец 'index', если он существует\n",
    "df_clav_fracture.drop(columns=['index'], inplace=True)\n",
    "\n",
    "# Выводим первые пять строк результата для проверки\n",
    "print(df_medimp.head())\n",
    "print(df_clav_fracture.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "IMG_SIZE = 224  # Размер изображения\n",
    "BATCH_SIZE = 16  # Размер пакета\n",
    "EPOCHS = 10  # Количество эпох\n",
    "\n",
    "# Шаг 1: Загрузка данных DICOM\n",
    "def load_dicom_images(df, img_size):\n",
    "    # Функция для загрузки изображений DICOM и их предобработки\n",
    "    images, labels = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        dicom_path = row['file_path']\n",
    "        label = row['pathology']\n",
    "        dicom = pydicom.dcmread(dicom_path)  # Чтение файла DICOM\n",
    "        img = dicom.pixel_array  # Извлечение массива пикселей\n",
    "        # Изменение размера изображения\n",
    "        img_resized = cv2.resize(img, (img_size, img_size))\n",
    "        # Преобразование изображения в формат RGB\n",
    "        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
    "        images.append(img_resized)\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Загрузка изображений и меток\n",
    "images_medimp, labels_medimp = load_dicom_images(df_medimp, IMG_SIZE)\n",
    "images_clav, labels_clav = load_dicom_images(df_clav_fracture, IMG_SIZE)\n",
    "\n",
    "# Объединение данных\n",
    "images = np.concatenate([images_medimp, images_clav])\n",
    "labels = np.concatenate([labels_medimp, labels_clav])\n",
    "\n",
    "# Шаг 2: Подготовка данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Аугментация данных\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,  # Диапазон вращения\n",
    "    width_shift_range=0.1,  # Диапазон сдвига по ширине\n",
    "    height_shift_range=0.1,  # Диапазон сдвига по высоте\n",
    "    horizontal_flip=True,  # Горизонтальное отражение\n",
    "    rescale=1./255  # Нормализация\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Нормализация тестовых данных\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Шаг 3: Построение модели\n",
    "base_model = DenseNet201(\n",
    "    weights='/Users/vadimkirsanov/Desktop/MIPT_DS/Python_coding_data/chest_xray_hac/saved_models/densenet201.keras', # укажите путь до актуальной модели\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# Тонкая настройка базовой модели\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Создание дополнительных слоев для модели\n",
    "dense = base_model.output\n",
    "dense = GlobalAveragePooling2D()(dense)\n",
    "\n",
    "# BatchNormalization и LeakyReLU\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = Dense(512)(dense)\n",
    "dense = LeakyReLU(alpha=0.1)(dense)\n",
    "\n",
    "# Механизм внимания (attention)\n",
    "attention_probs = Dense(512, activation='softmax', name='attention_probs')(dense)\n",
    "attention_mul = Multiply()([dense, attention_probs])\n",
    "\n",
    "# Слой Dropout\n",
    "dense = Dropout(0.5)(attention_mul)\n",
    "\n",
    "# Ещё один слой BatchNormalization и LeakyReLU\n",
    "dense = BatchNormalization()(dense)\n",
    "dense = Dense(256)(dense)\n",
    "dense = LeakyReLU(alpha=0.1)(dense)\n",
    "\n",
    "# Второй механизм внимания\n",
    "attention_probs_2 = Dense(256, activation='softmax', name='attention_probs_2')(dense)\n",
    "attention_mul_2 = Multiply()([dense, attention_probs_2])\n",
    "\n",
    "# Второй слой Dropout\n",
    "dense = Dropout(0.3)(attention_mul_2)\n",
    "\n",
    "# Выходной слой\n",
    "output = Dense(2, activation='softmax')(dense)\n",
    "\n",
    "# Компиляция модели\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Шаг 4: Обучение модели\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Шаг 5: Оценка модели\n",
    "predictions = model.predict(X_test)\n",
    "auc_score = roc_auc_score(y_test, predictions[:, 1])\n",
    "print(f\"AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Шаг 6: Постобработка\n",
    "def classify_output(predictions):\n",
    "    results = []\n",
    "    for pred in predictions:\n",
    "        foreign_bodies_prob = pred[0]\n",
    "        clavicle_fracture_prob = pred[1]\n",
    "        if foreign_bodies_prob < 0.5 and clavicle_fracture_prob < 0.5:\n",
    "            results.append(\"Normal\")\n",
    "        elif foreign_bodies_prob >= clavicle_fracture_prob:\n",
    "            results.append(\"Foreign Bodies\")\n",
    "        else:\n",
    "            results.append(\"Clavicle Fracture\")\n",
    "    return results\n",
    "\n",
    "results = classify_output(predictions)\n",
    "print(\"Classification Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_3.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
